{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-05-03 13:08:47 -04:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.4 s (started: 2021-05-03 13:08:47 -04:00)\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8021122"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 21.6 s (started: 2021-05-03 13:15:21 -04:00)\n"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.json('../yelp_academic_dataset_review.json')\n",
    "reviews = reviews.select(['business_id', 'text', 'stars'])\n",
    "reviews.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is 5.89 GB. Project requires a dataset of at least 500 MB. Dataset has ~8,000,000 rows. By the below calculations, a dataset ~10x smaller, or a dataset with ~700,000 rows will satisfy the 500 MB requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.78\n",
      "680910\n",
      "time: 0 ns (started: 2021-05-03 13:15:43 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# 5.89 / x = 0.5\n",
    "# 0.5 * x = 5.89\n",
    "x = 5.89 / 0.5\n",
    "print(x)\n",
    "print(int(8021122 / x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert stars column to string so that the logistic regression below treats one vs. five-stars as a classification rather than a regression column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms (started: 2021-05-03 13:15:43 -04:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "reviews = reviews.withColumn('stars', col('stars').cast(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data for Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-05-03 13:15:43 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# x = (1/10)\n",
    "# subset_df, large_df = reviews.randomSplit([x, 1 - x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-05-03 13:15:43 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# subset_df.coalesce(1).write.format('json').save('reviews_1-10.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Jupyter\n",
    "Merge the businesses and reviews datasets, filter by one or five-star reviews. This leaves us with ~1,000,000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.69 s (started: 2021-05-03 13:15:43 -04:00)\n"
     ]
    }
   ],
   "source": [
    "businesses = spark.read.json('../yelp_academic_dataset_business.json')\n",
    "businesses = businesses.select(['business_id', 'categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5055992"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11 s (started: 2021-05-03 13:15:44 -04:00)\n"
     ]
    }
   ],
   "source": [
    "restaurants = businesses.filter(businesses.categories.contains('Restaurants'))\n",
    "restaurant_reviews = reviews.join(restaurants, \"business_id\", \"inner\")\n",
    "restaurant_reviews = restaurant_reviews.select(['text', 'stars'])\n",
    "restaurant_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1256088"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.7 s (started: 2021-05-03 13:15:56 -04:00)\n"
     ]
    }
   ],
   "source": [
    "five_stars = restaurant_reviews.filter(restaurant_reviews.stars == 5.0)\n",
    "one_stars = restaurant_reviews.filter(restaurant_reviews.stars == 1.0)\n",
    "num_one_stars = one_stars.count()\n",
    "five_stars = five_stars.limit(num_one_stars)\n",
    "one_or_five_stars = five_stars.union(one_stars)\n",
    "num_one_stars * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert \"1\" and \"5\"-star reviews to 0 and 1 for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.8 s (started: 2021-05-03 13:16:08 -04:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol = 'stars', outputCol = 'categoryIndex')\n",
    "indexed = indexer.fit(one_or_five_stars).transform(one_or_five_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the below calculation, this one or five-stars dataset needs to be cut in about half to meet the 500 MB requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5420878155033724\n",
      "time: 0 ns (started: 2021-05-03 13:16:37 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# 680910 rows = ~500MB\n",
    "# 1256088 * x = 680910\n",
    "print(680910 / 1256088)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314693\n",
      "time: 38.8 s (started: 2021-05-03 13:16:37 -04:00)\n"
     ]
    }
   ],
   "source": [
    "x = 0.25\n",
    "subset_df, large_df = indexed.randomSplit([x, 1 - x])\n",
    "subset_count = subset_df.count()\n",
    "print(subset_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example review from the final subsetted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text='! The food is very tasty!  Every dish we have savored with my family has been simply spectacular!  Totally delighted with this place!\\n Oysters and aguachiles !!  Omg a delight !!!', stars='5.0', categoryIndex=1.0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22 s (started: 2021-05-03 13:17:16 -04:00)\n"
     ]
    }
   ],
   "source": [
    "subset_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "time: 12.6 s (started: 2021-05-03 13:17:38 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Build NLP preprocessing pipeline\n",
    "from sparknlp.base import DocumentAssembler\n",
    "document_assembler = DocumentAssembler() \\\n",
    ".setInputCol('text') \\\n",
    ".setOutputCol('document')\n",
    "from sparknlp.annotator import Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    ".setInputCols(['document']) \\\n",
    ".setOutputCol('tokenized') \\\n",
    ".setContextChars(['(', ')']) \\\n",
    ".setSplitChars(['-'])\n",
    "from sparknlp.annotator import Normalizer\n",
    "normalizer = Normalizer() \\\n",
    ".setInputCols(['tokenized']) \\\n",
    ".setOutputCol('normalized') \\\n",
    ".setLowercase(True) \\\n",
    ".setCleanupPatterns(['[^A-Za-z]'])\n",
    "from sparknlp.annotator import LemmatizerModel\n",
    "lemmatizer = LemmatizerModel \\\n",
    ".pretrained() \\\n",
    ".setInputCols(['normalized']) \\\n",
    ".setOutputCol('lemmatized')\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    ".setInputCols(['lemmatized']) \\\n",
    ".setOutputCol('unigrams') \\\n",
    ".setStopWords(nltk_stopwords)\n",
    "from sparknlp.annotator import NGramGenerator\n",
    "ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['unigrams']) \\\n",
    "    .setOutputCol('ngrams') \\\n",
    "    .setN(2) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')\n",
    "from sparknlp.base import Finisher\n",
    "finisher = Finisher() \\\n",
    ".setInputCols(['unigrams', 'ngrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-05-03 13:17:50 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Assemble pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline() \\\n",
    ".setStages([document_assembler,\n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            lemmatizer,\n",
    "            stopwords_cleaner,\n",
    "            ngrammer,\n",
    "            finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 594 ms (started: 2021-05-03 13:17:50 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Fit pipeline\n",
    "processed_reviews = pipeline.fit(subset_df).transform(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text='! The food is very tasty!  Every dish we have savored with my family has been simply spectacular!  Totally delighted with this place!\\n Oysters and aguachiles !!  Omg a delight !!!', stars='5.0', categoryIndex=1.0, finished_unigrams=['food', 'tasty', 'every', 'dish', 'savored', 'family', 'simply', 'spectacular', 'totally', 'delight', 'place', 'oyster', 'aguachiles', 'omg', 'delight'], finished_ngrams=['food', 'tasty', 'every', 'dish', 'savored', 'family', 'simply', 'spectacular', 'totally', 'delight', 'place', 'oyster', 'aguachiles', 'omg', 'delight', 'food_tasty', 'tasty_every', 'every_dish', 'dish_savored', 'savored_family', 'family_simply', 'simply_spectacular', 'spectacular_totally', 'totally_delight', 'delight_place', 'place_oyster', 'oyster_aguachiles', 'aguachiles_omg', 'omg_delight'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.1 s (started: 2021-05-03 13:17:51 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Examine one processed review\n",
    "processed_reviews.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-05-03 13:18:17 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "(trainingData, testData) = processed_reviews.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251754.40000000002\n",
      "time: 0 ns (started: 2021-05-03 13:18:17 -04:00)\n"
     ]
    }
   ],
   "source": [
    "trainingData_count = subset_count * 0.8\n",
    "print(trainingData_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24min 23s (started: 2021-05-03 13:18:17 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Count vectorization with minDF and maxDF parameters\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "tfizer = CountVectorizer(inputCol = 'finished_ngrams', outputCol = 'tf_features', \n",
    "                         minDF = 0.01, maxDF = 0.1, vocabSize = int(trainingData_count / 2))\n",
    "\n",
    "tf_model = tfizer.fit(trainingData)\n",
    "tf_result_training = tf_model.transform(trainingData)\n",
    "tf_result_test = tf_model.transform(testData)\n",
    "\n",
    "idfizer = IDF(inputCol = 'tf_features', outputCol = 'tfidf_features')\n",
    "\n",
    "idf_model = idfizer.fit(tf_result_training)\n",
    "tfidf_result_training = idf_model.transform(tf_result_training)\n",
    "tfidf_result_test = idf_model.transform(tf_result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 797 ms (started: 2021-05-03 16:55:40 -04:00)\n"
     ]
    }
   ],
   "source": [
    "tf_model.save(\"tfModel.model\")\n",
    "idf_model.save(\"idfModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 203 ms (started: 2021-05-03 17:11:55 -04:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizerModel\n",
    "tf_model = CountVectorizerModel.load(\"tfModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 172 ms (started: 2021-05-03 17:11:56 -04:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDFModel\n",
    "idf_model = IDFModel.load(\"idfModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "time: 0 ns (started: 2021-05-03 17:11:57 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Print vocablary length (i.e. # of columns)\n",
    "print(len(tf_model.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pizza',\n",
       " 'server',\n",
       " 'fry',\n",
       " 'meal',\n",
       " 'manager',\n",
       " 'way',\n",
       " 'sauce',\n",
       " 'sit',\n",
       " 'friend',\n",
       " 'call',\n",
       " 'find',\n",
       " 'bar',\n",
       " 'night',\n",
       " 'two',\n",
       " 'thing',\n",
       " 'hour',\n",
       " 'much',\n",
       " 'location',\n",
       " 'serve',\n",
       " 'salad',\n",
       " 'day',\n",
       " 'fresh',\n",
       " 'walk',\n",
       " 'cheese',\n",
       " 'everything',\n",
       " 'right',\n",
       " 'dish',\n",
       " 'still',\n",
       " 'burger',\n",
       " 'long',\n",
       " 'work',\n",
       " 'another',\n",
       " 'pay',\n",
       " 'dinner',\n",
       " 'feel',\n",
       " 'seat',\n",
       " 'cook',\n",
       " 'review',\n",
       " 'check',\n",
       " 'need',\n",
       " 'little',\n",
       " 'bring',\n",
       " 'waitress',\n",
       " 'come_back',\n",
       " 'new',\n",
       " 'visit',\n",
       " 'around',\n",
       " 'sure',\n",
       " 'last',\n",
       " 'star',\n",
       " 'lunch',\n",
       " 'cant',\n",
       " 'every',\n",
       " 'sandwich',\n",
       " 'use',\n",
       " 'go_back',\n",
       " 'side',\n",
       " 'meat',\n",
       " 'wasnt',\n",
       " 'next',\n",
       " 'since',\n",
       " 'open',\n",
       " 'vegas',\n",
       " 'small',\n",
       " 'year',\n",
       " 'something',\n",
       " 'favorite',\n",
       " 'many',\n",
       " 'top',\n",
       " 'close',\n",
       " 'quality',\n",
       " 'late',\n",
       " 'lot',\n",
       " 'flavor',\n",
       " 'enjoy',\n",
       " 'rice',\n",
       " 'start',\n",
       " 'away',\n",
       " 'sushi',\n",
       " 'worth',\n",
       " 'room',\n",
       " 'hot',\n",
       " 'seem',\n",
       " 'pretty',\n",
       " 'cold',\n",
       " 'decide',\n",
       " 'roll',\n",
       " 'taco',\n",
       " 'customer_service',\n",
       " 'super',\n",
       " 'plate',\n",
       " 'area',\n",
       " 'put',\n",
       " 'return',\n",
       " 'family',\n",
       " 'owner',\n",
       " 'waiter',\n",
       " 'excellent',\n",
       " 'awesome',\n",
       " 'money',\n",
       " 'let',\n",
       " 'horrible',\n",
       " 'nothing',\n",
       " 'big',\n",
       " 'home',\n",
       " 'rude',\n",
       " 'end',\n",
       " 'disappointed',\n",
       " 'soup',\n",
       " 'steak',\n",
       " 'beer',\n",
       " 'water',\n",
       " 'special',\n",
       " 'happy',\n",
       " 'bread',\n",
       " 'stop',\n",
       " 'breakfast',\n",
       " 'keep',\n",
       " 'offer',\n",
       " 'guy',\n",
       " 'finally',\n",
       " 'busy',\n",
       " 'half',\n",
       " 'husband',\n",
       " 'anything',\n",
       " 'old',\n",
       " 'clean',\n",
       " 'arrive',\n",
       " 'terrible',\n",
       " 'drive',\n",
       " 'egg',\n",
       " 'business',\n",
       " 'beef',\n",
       " 'else',\n",
       " 'different',\n",
       " 'charge',\n",
       " 'wont',\n",
       " 'atmosphere',\n",
       " 'portion',\n",
       " 'enough',\n",
       " 'wrong',\n",
       " 'point',\n",
       " 'stay',\n",
       " 'min',\n",
       " 'spot',\n",
       " 'coffee',\n",
       " 'care',\n",
       " 'sweet',\n",
       " 'absolutely',\n",
       " 'though',\n",
       " 'perfect',\n",
       " 'line',\n",
       " 'full',\n",
       " 'thank',\n",
       " 'item',\n",
       " 'pm',\n",
       " 'youre',\n",
       " 'dessert',\n",
       " 'couldnt',\n",
       " 'employee',\n",
       " 'first_time',\n",
       " 'bit',\n",
       " 'option',\n",
       " 'shrimp',\n",
       " 'large',\n",
       " 'everyone',\n",
       " 'fish',\n",
       " 'party',\n",
       " 'actually',\n",
       " 'expect',\n",
       " 'free',\n",
       " 'without',\n",
       " 'show',\n",
       " 'far',\n",
       " 'today',\n",
       " 'bill',\n",
       " 'ok',\n",
       " 'pork',\n",
       " 'fast',\n",
       " 'least',\n",
       " 'wing',\n",
       " 'front',\n",
       " 'hard',\n",
       " 'inside',\n",
       " 'hand',\n",
       " 'someone',\n",
       " 'kid',\n",
       " 'must',\n",
       " 'almost',\n",
       " 'person',\n",
       " 'receive',\n",
       " 'wife',\n",
       " 'appetizer',\n",
       " 'run',\n",
       " 'spicy',\n",
       " 'week',\n",
       " 'potato',\n",
       " 'ice',\n",
       " 'change',\n",
       " 'name',\n",
       " 'highly',\n",
       " 'talk',\n",
       " 'add',\n",
       " 'however',\n",
       " 'maybe',\n",
       " 'town',\n",
       " 'tasty',\n",
       " 'second',\n",
       " 'pick',\n",
       " 'ill',\n",
       " 'probably',\n",
       " 'kind',\n",
       " 'piece',\n",
       " 'la',\n",
       " 'noodle',\n",
       " 'dining',\n",
       " 'high',\n",
       " 'dry',\n",
       " 'extremely',\n",
       " 'door',\n",
       " 'extra',\n",
       " 'kitchen',\n",
       " 'help',\n",
       " 'couple',\n",
       " 'house',\n",
       " 'reservation',\n",
       " 'wine',\n",
       " 'instead',\n",
       " 'live',\n",
       " 'cream',\n",
       " 'huge',\n",
       " 'chef',\n",
       " 'tip',\n",
       " 'whole',\n",
       " 'three',\n",
       " 'group',\n",
       " 'bartender',\n",
       " 'outside',\n",
       " 'bowl',\n",
       " 'bite',\n",
       " 'happen',\n",
       " 'hear',\n",
       " 'tea',\n",
       " 'hotel',\n",
       " 'fantastic',\n",
       " 'doesnt',\n",
       " 'chip',\n",
       " 'grill',\n",
       " 'oh',\n",
       " 'highly_recommend',\n",
       " 'taste_like',\n",
       " 'speak',\n",
       " 'anyone',\n",
       " 'girl',\n",
       " 'wonderful',\n",
       " 'usually',\n",
       " 'phone',\n",
       " 'please',\n",
       " 'forget',\n",
       " 'slow',\n",
       " 'poor',\n",
       " 'id',\n",
       " 'finish',\n",
       " 'selection',\n",
       " 'buffet',\n",
       " 'stand',\n",
       " 'spend',\n",
       " 'ive_ever',\n",
       " 'already',\n",
       " 'several',\n",
       " 'lady',\n",
       " 'treat',\n",
       " 'choice',\n",
       " 'part',\n",
       " 'course',\n",
       " 'size',\n",
       " 'hope',\n",
       " 'either',\n",
       " 'quite',\n",
       " 'quick',\n",
       " 'great_food',\n",
       " 'fill',\n",
       " 'park',\n",
       " 'turn',\n",
       " 'cake',\n",
       " 'overall',\n",
       " 'store',\n",
       " 'include',\n",
       " 'yelp',\n",
       " 'mean',\n",
       " 'onion',\n",
       " 'green',\n",
       " 'thai',\n",
       " 'card',\n",
       " 'attentive',\n",
       " 'yet',\n",
       " 'waste',\n",
       " 'fun',\n",
       " 'less',\n",
       " 'especially',\n",
       " 'wish',\n",
       " 'move',\n",
       " 'warm',\n",
       " 'wouldnt',\n",
       " 'watch',\n",
       " 'bacon',\n",
       " 'miss',\n",
       " 'bean',\n",
       " 'empty',\n",
       " 'problem',\n",
       " 'choose',\n",
       " 'ready',\n",
       " 'hostess',\n",
       " 'literally',\n",
       " 'understand',\n",
       " 'completely',\n",
       " 'rib',\n",
       " 'take_order',\n",
       " 'yes',\n",
       " 'reason',\n",
       " 'mexican',\n",
       " 'love_place',\n",
       " 'glass',\n",
       " 'deal',\n",
       " 'counter',\n",
       " 'regular',\n",
       " 'surprise',\n",
       " 'great_service',\n",
       " 'management',\n",
       " 'burrito',\n",
       " 'look_like',\n",
       " 'strip',\n",
       " 'mention',\n",
       " 'soon',\n",
       " 'write',\n",
       " 'bbq',\n",
       " 'food_good',\n",
       " 'local',\n",
       " 'dirty',\n",
       " 'real',\n",
       " 'notice',\n",
       " 'share',\n",
       " 'guess',\n",
       " 'light',\n",
       " 'feel_like',\n",
       " 'awful',\n",
       " 'birthday',\n",
       " 'isnt',\n",
       " 'stuff',\n",
       " 'make_sure',\n",
       " 'cheap',\n",
       " 'red',\n",
       " 'fine',\n",
       " 'attitude',\n",
       " 'get_food',\n",
       " 'delivery',\n",
       " 'amount',\n",
       " 'sorry',\n",
       " 'food_great',\n",
       " 'greet',\n",
       " 'ago',\n",
       " 'life',\n",
       " 'believe',\n",
       " 'cool',\n",
       " 'entire',\n",
       " 'pasta',\n",
       " 'cut',\n",
       " 'street',\n",
       " 'buy',\n",
       " 'wait_minute',\n",
       " 'bland',\n",
       " 'month',\n",
       " 'slice',\n",
       " 'issue',\n",
       " 'man',\n",
       " 'wow',\n",
       " 'chinese',\n",
       " 'game',\n",
       " 'twice',\n",
       " 'explain',\n",
       " 'never_go',\n",
       " 'deliver',\n",
       " 'read',\n",
       " 'establishment',\n",
       " 'really_good',\n",
       " 'every_time',\n",
       " 'able',\n",
       " 'send',\n",
       " 'perfectly',\n",
       " 'decent',\n",
       " 'prepare',\n",
       " 'may',\n",
       " 'good_food',\n",
       " 'job',\n",
       " 'crab',\n",
       " 'throw',\n",
       " 'list',\n",
       " 'okay',\n",
       " 'music',\n",
       " 'short',\n",
       " 'authentic',\n",
       " 'mind',\n",
       " 'style',\n",
       " 'tonight',\n",
       " 'plus',\n",
       " 'tomato',\n",
       " 'brunch',\n",
       " 'la_vegas',\n",
       " 'behind',\n",
       " 'lack',\n",
       " 'white',\n",
       " 'french',\n",
       " 'crispy',\n",
       " 'theyre',\n",
       " 'dont_know',\n",
       " 'shop',\n",
       " 'garlic',\n",
       " 'fact',\n",
       " 'chocolate',\n",
       " 'sunday',\n",
       " 'italian',\n",
       " 'ingredient',\n",
       " 'dog',\n",
       " 'boyfriend',\n",
       " 'play',\n",
       " 'mix',\n",
       " 'past',\n",
       " 'next_time',\n",
       " 'welcome',\n",
       " 'early',\n",
       " 'burn',\n",
       " 'season',\n",
       " 'seriously',\n",
       " 'food_service',\n",
       " 'saturday',\n",
       " 'cup',\n",
       " 'complain',\n",
       " 'might',\n",
       " 'even_though',\n",
       " 'disgusting',\n",
       " 'toast',\n",
       " 'set',\n",
       " 'hungry',\n",
       " 'happy_hour',\n",
       " 'guest',\n",
       " 'trip',\n",
       " 'quickly',\n",
       " 'salsa',\n",
       " 'vegan',\n",
       " 'head',\n",
       " 'helpful',\n",
       " 'salmon',\n",
       " 'sign',\n",
       " 'decor',\n",
       " 'smell',\n",
       " 'type',\n",
       " 'service_great',\n",
       " 'box',\n",
       " 'morning',\n",
       " 'didnt_even',\n",
       " 'cocktail',\n",
       " 'fan',\n",
       " 'totally',\n",
       " 'recommend_place',\n",
       " 'patio',\n",
       " 'request',\n",
       " 'tender',\n",
       " 'state',\n",
       " 'pull',\n",
       " 'simple',\n",
       " 'rest',\n",
       " 'remember',\n",
       " 'weekend',\n",
       " 'butter',\n",
       " 'save',\n",
       " 'floor',\n",
       " 'disappointing',\n",
       " 'yummy',\n",
       " 'expensive',\n",
       " 'refill',\n",
       " 'pepper',\n",
       " 'lobster',\n",
       " 'black',\n",
       " 'dollar',\n",
       " 'dress',\n",
       " 'suck',\n",
       " 'together',\n",
       " 'cost',\n",
       " 'honestly',\n",
       " 'immediately',\n",
       " 'pass',\n",
       " 'often',\n",
       " 'mess',\n",
       " 'rate',\n",
       " 'never_come',\n",
       " 'one_good',\n",
       " 'great_place',\n",
       " 'mouth',\n",
       " 'meet',\n",
       " 'last_time',\n",
       " 'werent',\n",
       " 'bad_service',\n",
       " 'stick',\n",
       " 'rather',\n",
       " 'impressed',\n",
       " 'avoid',\n",
       " 'seafood',\n",
       " 'chance',\n",
       " 'food_come',\n",
       " 'ice_cream',\n",
       " 'although',\n",
       " 'eye',\n",
       " 'suppose',\n",
       " 'apologize',\n",
       " 'thru',\n",
       " 'curry',\n",
       " 'minute_late',\n",
       " 'beautiful',\n",
       " 'across',\n",
       " 'grab',\n",
       " 'friday',\n",
       " 'lose',\n",
       " 'answer',\n",
       " 'base',\n",
       " 'pack',\n",
       " 'mistake',\n",
       " 'word',\n",
       " 'date',\n",
       " 'medium',\n",
       " 'window',\n",
       " 'sad',\n",
       " 'mushroom',\n",
       " 'bag',\n",
       " 'crust',\n",
       " 'havent',\n",
       " 'mac',\n",
       " 'bun',\n",
       " 'near',\n",
       " 'place_go',\n",
       " 'unfortunately',\n",
       " 'bottle',\n",
       " 'salt',\n",
       " 'main',\n",
       " 'weve',\n",
       " 'low',\n",
       " 'wrap',\n",
       " 'smoke',\n",
       " 'available',\n",
       " 'break',\n",
       " 'variety',\n",
       " 'anyway',\n",
       " 'pancake',\n",
       " 'place_order',\n",
       " 'excited',\n",
       " 'car',\n",
       " 'glad',\n",
       " 'five',\n",
       " 'freeze',\n",
       " 'cashier',\n",
       " 'number',\n",
       " 'four',\n",
       " 'n',\n",
       " 'mediocre',\n",
       " 'question',\n",
       " 'daughter',\n",
       " 'reasonable',\n",
       " 'woman',\n",
       " 'suggest',\n",
       " 'order_food',\n",
       " 'clearly',\n",
       " 'provide',\n",
       " 'son',\n",
       " 'total',\n",
       " 'would_recommend',\n",
       " 'flavorful',\n",
       " 'worker',\n",
       " 'vegetable',\n",
       " 'last_night',\n",
       " 'salty',\n",
       " 'gross',\n",
       " 'dip',\n",
       " 'attention',\n",
       " 'consider',\n",
       " 'due',\n",
       " 'plan',\n",
       " 'easy',\n",
       " 'realize',\n",
       " 'crowd',\n",
       " 'time_go',\n",
       " 'hold',\n",
       " 'spice',\n",
       " 'take_minute',\n",
       " 'young',\n",
       " 'plenty',\n",
       " 'sausage',\n",
       " 'lettuce',\n",
       " 'good_service',\n",
       " 'simply',\n",
       " 'smile',\n",
       " 'hit',\n",
       " 'begin',\n",
       " 'average',\n",
       " 'bad_experience',\n",
       " 'overpriced',\n",
       " 'bother',\n",
       " 'vegetarian',\n",
       " 'single',\n",
       " 'along',\n",
       " 'cant_wait',\n",
       " 'world',\n",
       " 'hair',\n",
       " 'face',\n",
       " 'refund',\n",
       " 'city',\n",
       " 'ignore',\n",
       " 'picture',\n",
       " 'long_time',\n",
       " 'ambiance',\n",
       " 'upon',\n",
       " 'zero',\n",
       " 'soft',\n",
       " 'forward',\n",
       " 'continue',\n",
       " 'cannot',\n",
       " 'drive_thru',\n",
       " 'loud',\n",
       " 'get_order',\n",
       " 'hang',\n",
       " 'try_place',\n",
       " 'tuna',\n",
       " 'bathroom',\n",
       " 'definitely_back',\n",
       " 'sick',\n",
       " 'barely',\n",
       " 'idea',\n",
       " 'service_food',\n",
       " 'cafe',\n",
       " 'dine',\n",
       " 'wall',\n",
       " 'incredible',\n",
       " 'soggy',\n",
       " 'corn',\n",
       " 'quality_food',\n",
       " 'truly',\n",
       " 'combo',\n",
       " 'die',\n",
       " 'accommodate',\n",
       " 'rush',\n",
       " 'fix',\n",
       " 'beyond',\n",
       " 'somewhere',\n",
       " 'chili',\n",
       " 'figure',\n",
       " 'brown',\n",
       " 'th',\n",
       " 'credit',\n",
       " 'cover',\n",
       " 'mom',\n",
       " 'staff_friendly',\n",
       " 'drop',\n",
       " 'basically',\n",
       " 'fall',\n",
       " 'bake',\n",
       " 'online',\n",
       " 'interest',\n",
       " 'youll',\n",
       " 'etc',\n",
       " 'toppings',\n",
       " 'note',\n",
       " 'post',\n",
       " 'outstanding',\n",
       " 'cash',\n",
       " 'seem_like',\n",
       " 'obviously',\n",
       " 'greasy',\n",
       " 'book',\n",
       " 'sour',\n",
       " 'joint',\n",
       " 'unique',\n",
       " 'become',\n",
       " 'ridiculous',\n",
       " 'melt',\n",
       " 'joke',\n",
       " 'allow',\n",
       " 'good_thing',\n",
       " 'forever',\n",
       " 'raw',\n",
       " 'margarita',\n",
       " 'level',\n",
       " 'gem',\n",
       " 'tiny',\n",
       " 'follow',\n",
       " 'definitely_come',\n",
       " 'view',\n",
       " 'thin',\n",
       " 'give_place',\n",
       " 'phoenix',\n",
       " 'matter',\n",
       " 'hate',\n",
       " 'anywhere',\n",
       " 'refuse',\n",
       " 'veggies',\n",
       " 'correct',\n",
       " 'complaint',\n",
       " 'im_sure',\n",
       " 'multiple',\n",
       " 'clear',\n",
       " 'oil',\n",
       " 'pleasant',\n",
       " 'unless',\n",
       " 'cause',\n",
       " 'recently',\n",
       " 'entrees',\n",
       " 'space',\n",
       " 'healthy',\n",
       " 'crave',\n",
       " 'inform',\n",
       " 'say_would',\n",
       " 'drink_order',\n",
       " 'make_reservation',\n",
       " 'apparently',\n",
       " 'vibe',\n",
       " 'food_amazing',\n",
       " 'lol',\n",
       " 'disappointment',\n",
       " 'much_well',\n",
       " 'sound',\n",
       " 'one_star',\n",
       " 'fast_food',\n",
       " 'food_delicious',\n",
       " 'within',\n",
       " 'entree',\n",
       " 'standard',\n",
       " 'minute_get',\n",
       " 'touch',\n",
       " 'non',\n",
       " 'heat',\n",
       " 'middle',\n",
       " 'service_good',\n",
       " 'sell',\n",
       " 'upset',\n",
       " 'dining_experience',\n",
       " 'rock',\n",
       " 'per',\n",
       " 'whatever',\n",
       " 'value',\n",
       " 'look_forward',\n",
       " 'ive_never',\n",
       " 'didnt_want',\n",
       " 'time_come',\n",
       " 'cute',\n",
       " 'make_feel',\n",
       " 'recommendation',\n",
       " 'nasty',\n",
       " 'negative',\n",
       " 'ahead',\n",
       " 'downtown',\n",
       " 'waste_time',\n",
       " 'homemade',\n",
       " 'case',\n",
       " 'one_favorite',\n",
       " 'apology',\n",
       " 'tasteless',\n",
       " 'wait_staff',\n",
       " 'go_place',\n",
       " 'anymore',\n",
       " 'appreciate',\n",
       " 'deserve',\n",
       " 'excuse',\n",
       " 'update',\n",
       " 'none',\n",
       " 'exactly',\n",
       " 'complete',\n",
       " 'except',\n",
       " 'sit_bar',\n",
       " 'locate',\n",
       " 'could_give',\n",
       " 'place_eat',\n",
       " 'compare',\n",
       " 'pretty_good',\n",
       " 'wonder',\n",
       " 'elsewhere',\n",
       " 'arent',\n",
       " 'good_place',\n",
       " 'would_definitely',\n",
       " 'dont_think',\n",
       " 'friendly_staff']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2021-05-03 17:11:58 -04:00)\n"
     ]
    }
   ],
   "source": [
    "tf_model.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12min 32s (started: 2021-05-03 14:05:09 -04:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "num_topics = 10\n",
    "max_iter = 10\n",
    "lda = LDA(k = num_topics, \n",
    "          maxIter = max_iter, \n",
    "          featuresCol = 'tfidf_features')\n",
    "ldaModel = lda.fit(tfidf_result_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-05-03 14:20:19 -04:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------------------------------------------+\n",
      "|topic|                                                                                          topicWords|\n",
      "+-----+----------------------------------------------------------------------------------------------------+\n",
      "|    0|                               [business, rude, sandwich, owner, work, walk, server, bar, seem, sit]|\n",
      "|    1|                         [taco, burrito, dog, salsa, chip, mexican, waste, bean, burger, happy_hour]|\n",
      "|    2|                           [call, manager, waitress, pizza, charge, pay, another, phone, hour, bill]|\n",
      "|    3|                [drive, every, every_time, thru, thai, drive_thru, location, burger, curry, time_go]|\n",
      "|    4|[awesome, highly, highly_recommend, perfect, tea, favorite, atmosphere, everything, great_service...|\n",
      "|    5|                                       [beer, bar, sit, night, last, game, busy, around, hair, walk]|\n",
      "|    6|                                 [steak, wine, server, cake, salad, dessert, meal, rib, bread, crab]|\n",
      "|    7|                            [pizza, egg, wing, salad, toast, cheese, breakfast, star, sauce, coffee]|\n",
      "|    8|                                   [sushi, fry, burger, rice, noodle, dish, soup, sauce, bowl, roll]|\n",
      "|    9|                            [room, la, vegas, hotel, buffet, stay, la_vegas, reservation, hour, day]|\n",
      "+-----+----------------------------------------------------------------------------------------------------+\n",
      "\n",
      "time: 4.81 s (started: 2021-05-03 14:20:21 -04:00)\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 10\n",
    "topics = lda_model \\\n",
    ".describeTopics(num_top_words) \\\n",
    ".withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 62 ms (started: 2021-05-03 14:20:44 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Define logistic regression with ridge\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'tfidf_features', labelCol = 'categoryIndex', \n",
    "                        family = 'binomial', elasticNetParam = 0, regParam = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.5,\n",
       " 'aggregationDepth': 2,\n",
       " 'standardization': True,\n",
       " 'fitIntercept': True,\n",
       " 'elasticNetParam': 0.0,\n",
       " 'predictionCol': 'prediction',\n",
       " 'featuresCol': 'tfidf_features',\n",
       " 'labelCol': 'categoryIndex',\n",
       " 'rawPredictionCol': 'rawPrediction',\n",
       " 'probabilityCol': 'probability',\n",
       " 'maxIter': 100,\n",
       " 'regParam': 0.1,\n",
       " 'tol': 1e-06,\n",
       " 'family': 'binomial'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-05-03 14:20:44 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Print all parameters\n",
    "{param[0].name: param[1] for param in lr.extractParamMap().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11min 52s (started: 2021-05-03 14:20:44 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Fit LR model\n",
    "lrModel = lr.fit(tfidfResult_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 438 ms (started: 2021-05-03 16:55:23 -04:00)\n"
     ]
    }
   ],
   "source": [
    "lrModel.save(\"lrModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 250 ms (started: 2021-05-03 16:59:58 -04:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "lrModel = LogisticRegressionModel.load(\"lrModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 63 ms (started: 2021-05-03 14:32:36 -04:00)\n"
     ]
    }
   ],
   "source": [
    "lrPredictions_training = lrModel.transform(tfidf_result_training)\n",
    "lrPredictions_test = lrModel.transform(tfidf_result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32 ms (started: 2021-05-03 14:32:37 -04:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = 'categoryIndex', predictionCol = 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23min 46s (started: 2021-05-03 14:32:37 -04:00)\n"
     ]
    }
   ],
   "source": [
    "acc_training_lr = evaluator.evaluate(lrPredictions_training, {evaluator.metricName: \"accuracy\"})\n",
    "acc_test_lr = evaluator.evaluate(lrPredictions_test, {evaluator.metricName: \"accuracy\"})\n",
    "# f1 = evaluator.evaluate(lrPredictions_training, {evaluator.metricName: \"f1\"})\n",
    "# weightedPrecision = evaluator.evaluate(lrPredictions_training, {evaluator.metricName: \"weightedPrecision\"})\n",
    "# weightedRecall = evaluator.evaluate(lrPredictions_training, {evaluator.metricName: \"weightedRecall\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9409918392969241\n",
      "Test accuracy: 0.9402371466213749\n",
      "time: 0 ns (started: 2021-05-03 14:56:23 -04:00)\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy: ' + str(acc_training_lr))\n",
    "print('Test accuracy: ' + str(acc_test_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2021-05-03 14:56:23 -04:00)\n"
     ]
    }
   ],
   "source": [
    "coef_matrix = lrModel.coefficientMatrix\n",
    "coef_list = coef_matrix.toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-0.219863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>-0.215739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>-0.196160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-0.193927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>-0.182215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.193411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0.193799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0.198794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.224351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.237011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>787 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "101 -0.219863\n",
       "128 -0.215739\n",
       "476 -0.196160\n",
       "105 -0.193927\n",
       "370 -0.182215\n",
       "..        ...\n",
       "66   0.193411\n",
       "244  0.193799\n",
       "321  0.198794\n",
       "97   0.224351\n",
       "98   0.237011\n",
       "\n",
       "[787 rows x 1 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 859 ms (started: 2021-05-03 14:56:23 -04:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(coef_list).T.sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horrible\n",
      "terrible\n",
      "disappointing\n",
      "rude\n",
      "bland\n",
      "mediocre\n",
      "bad_service\n",
      "awful\n",
      "disgusting\n",
      "overpriced\n",
      "never_go\n",
      "poor\n",
      "suck\n",
      "never_come\n",
      "one_star\n",
      "tasteless\n",
      "gross\n",
      "disappointment\n",
      "waste\n",
      "slow\n",
      "time: 219 ms (started: 2021-05-03 14:56:24 -04:00)\n"
     ]
    }
   ],
   "source": [
    "coef_df = pd.DataFrame(coef_list).T.sort_values(0, ascending = True)\n",
    "for i in range(0, 20):\n",
    "    print(tf_model.vocabulary[coef_df.index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awesome\n",
      "excellent\n",
      "love_place\n",
      "fantastic\n",
      "favorite\n",
      "definitely_back\n",
      "food_amazing\n",
      "cant_wait\n",
      "perfect\n",
      "food_delicious\n",
      "definitely_come\n",
      "highly_recommend\n",
      "great_food\n",
      "one_good\n",
      "great_service\n",
      "gem\n",
      "friendly_staff\n",
      "wonderful\n",
      "attentive\n",
      "yummy\n",
      "time: 172 ms (started: 2021-05-03 14:56:24 -04:00)\n"
     ]
    }
   ],
   "source": [
    "coef_df = pd.DataFrame(coef_list).T.sort_values(0, ascending = False)\n",
    "for i in range(0, 20):\n",
    "    print(tf_model.vocabulary[coef_df.index[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 62 ms (started: 2021-05-03 15:37:25 -04:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'tfidf_features', labelCol = 'categoryIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 5867653639892386317,\n",
       " 'maxDepth': 5,\n",
       " 'maxBins': 32,\n",
       " 'minInstancesPerNode': 1,\n",
       " 'minInfoGain': 0.0,\n",
       " 'maxMemoryInMB': 256,\n",
       " 'cacheNodeIds': False,\n",
       " 'checkpointInterval': 10,\n",
       " 'impurity': 'gini',\n",
       " 'numTrees': 20,\n",
       " 'featureSubsetStrategy': 'auto',\n",
       " 'subsamplingRate': 1.0,\n",
       " 'leafCol': '',\n",
       " 'minWeightFractionPerNode': 0.0,\n",
       " 'bootstrap': True,\n",
       " 'predictionCol': 'prediction',\n",
       " 'featuresCol': 'tfidf_features',\n",
       " 'labelCol': 'categoryIndex',\n",
       " 'rawPredictionCol': 'rawPrediction',\n",
       " 'probabilityCol': 'probability'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2021-05-03 15:37:25 -04:00)\n"
     ]
    }
   ],
   "source": [
    "{param[0].name: param[1] for param in rf.extractParamMap().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 43min 39s (started: 2021-05-03 15:37:25 -04:00)\n"
     ]
    }
   ],
   "source": [
    "rfModel = rf.fit(tfidf_result_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 94 ms (started: 2021-05-03 16:21:04 -04:00)\n"
     ]
    }
   ],
   "source": [
    "rfPredictions_training = rfModel.transform(tfidf_result_training)\n",
    "rfPredictions_test = rfModel.transform(tfidf_result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28min 45s (started: 2021-05-03 16:21:05 -04:00)\n"
     ]
    }
   ],
   "source": [
    "acc_training_rf = evaluator.evaluate(rfPredictions_training, {evaluator.metricName: \"accuracy\"})\n",
    "acc_test_rf = evaluator.evaluate(rfPredictions_test, {evaluator.metricName: \"accuracy\"})\n",
    "# f1 = evaluator.evaluate(rfPredictions_training, {evaluator.metricName: \"f1\"})\n",
    "# weightedPrecision = evaluator.evaluate(rfPredictions_training, {evaluator.metricName: \"weightedPrecision\"})\n",
    "# weightedRecall = evaluator.evaluate(rfPredictions_training, {evaluator.metricName: \"weightedRecall\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8126494870755759\n",
      "Test accuracy: 0.8103779425070239\n",
      "time: 15 ms (started: 2021-05-03 16:49:51 -04:00)\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy: ' + str(acc_training_rf))\n",
    "print('Test accuracy: ' + str(acc_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 78 ms (started: 2021-05-03 16:49:51 -04:00)\n"
     ]
    }
   ],
   "source": [
    "coef_matrix = rfModel.featureImportances\n",
    "coef_list = coef_matrix.toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rude\n",
      "perfect\n",
      "excellent\n",
      "horrible\n",
      "call\n",
      "pay\n",
      "awesome\n",
      "another\n",
      "terrible\n",
      "someone\n",
      "waste\n",
      "cold\n",
      "fresh\n",
      "waitress\n",
      "highly_recommend\n",
      "charge\n",
      "fantastic\n",
      "attentive\n",
      "bland\n",
      "min\n",
      "money\n",
      "attitude\n",
      "nothing\n",
      "great_food\n",
      "bill\n",
      "favorite\n",
      "ok\n",
      "never_come\n",
      "perfectly\n",
      "wrong\n",
      "atmosphere\n",
      "ignore\n",
      "arrive\n",
      "taste_like\n",
      "disappointing\n",
      "receive\n",
      "seem\n",
      "disgusting\n",
      "yummy\n",
      "highly\n",
      "time: 375 ms (started: 2021-05-03 16:49:51 -04:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "coef_df = pd.DataFrame(coef_list).sort_values(0, ascending = False)\n",
    "for i in range(0, 40):\n",
    "    print(tf_model.vocabulary[coef_df.index[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
