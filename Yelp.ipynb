{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-04-22 13:10:23 -04:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.5 s (started: 2021-04-22 13:10:23 -04:00)\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8021122"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 37.8 s (started: 2021-04-22 13:10:34 -04:00)\n"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.json('yelp_academic_dataset_review.json')\n",
    "reviews = reviews.select(['business_id', 'text', 'stars'])\n",
    "reviews.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is 5.89 GB. Project requires a dataset of at least 500 MB. Dataset has ~8,000,000 rows. By the below calculations, a dataset ~10x smaller, or a dataset with ~700,000 rows will satisfy the 500 MB requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.78\n",
      "680910\n",
      "time: 0 ns (started: 2021-04-22 13:11:12 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# 5.89 / x = 0.5\n",
    "# 0.5 * x = 5.89\n",
    "x = 5.89 / 0.5\n",
    "print(x)\n",
    "print(int(8021122 / x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert stars column to string so that the logistic regression below treats one vs. five-stars as a classification rather than a regression column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms (started: 2021-04-22 13:11:12 -04:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "reviews = reviews.withColumn('stars', col('stars').cast(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data for Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-04-22 13:11:12 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# x = (1/10)\n",
    "# subset_df, large_df = reviews.randomSplit([x, 1 - x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-04-22 13:11:12 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# subset_df.coalesce(1).write.format('json').save('reviews_1-10.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Jupyter\n",
    "Merge the businesses and reviews datasets, filter by one or five-star reviews. This leaves us with ~1,000,000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.61 s (started: 2021-04-22 13:11:12 -04:00)\n"
     ]
    }
   ],
   "source": [
    "businesses = spark.read.json('yelp_academic_dataset_business.json')\n",
    "businesses = businesses.select(['business_id', 'categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5055992"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.88 s (started: 2021-04-22 13:11:13 -04:00)\n"
     ]
    }
   ],
   "source": [
    "restaurants = businesses.filter(businesses.categories.contains('Restaurants'))\n",
    "restaurant_reviews = reviews.join(restaurants, \"business_id\", \"inner\")\n",
    "restaurant_reviews = restaurant_reviews.select(['text', 'stars'])\n",
    "restaurant_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1256088"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.62 s (started: 2021-04-22 13:11:22 -04:00)\n"
     ]
    }
   ],
   "source": [
    "five_stars = restaurant_reviews.filter(restaurant_reviews.stars == 5.0)\n",
    "one_stars = restaurant_reviews.filter(restaurant_reviews.stars == 1.0)\n",
    "num_one_stars = one_stars.count()\n",
    "five_stars = five_stars.limit(num_one_stars)\n",
    "one_or_five_stars = five_stars.union(one_stars)\n",
    "num_one_stars * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert \"1\" and \"5\"-star reviews to 0 and 1 for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18.9 s (started: 2021-04-22 13:11:32 -04:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol = 'stars', outputCol = 'categoryIndex')\n",
    "indexed = indexer.fit(one_or_five_stars).transform(one_or_five_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the below calculation, this one or five-stars dataset needs to be cut in about half to meet the 500 MB requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5420878155033724\n",
      "time: 0 ns (started: 2021-04-22 13:11:51 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# 680910 rows = ~500MB\n",
    "# 1256088 * x = 680910\n",
    "print(680910 / 1256088)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314343\n",
      "time: 28.7 s (started: 2021-04-22 13:11:51 -04:00)\n"
     ]
    }
   ],
   "source": [
    "x = 0.25\n",
    "subset_df, large_df = indexed.randomSplit([x, 1 - x])\n",
    "subset_count = subset_df.count()\n",
    "print(subset_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example review from the final subsetted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text=\"!!!!!\\n\\nWe found ourselves at Foxleys because we were visiting friends in town and they mentioned they had been wanting to try it. I'm so glad they waited!\\n\\nEverything we ordered was *excellent* (though some more than others - get the kale salad and the side ribs for sure), the wine selection was so so - we chose a moderately priced bottle of red which was fine (but apparently forgettable), and the service was friendly, but the waitress/bartender was seemingly (and understandably) overwhelmed.\\n\\nWe are trying to find a way to visit again so we can repeat the night and try the rest of the menu :)\", stars='5.0', categoryIndex=1.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.7 s (started: 2021-04-22 13:12:20 -04:00)\n"
     ]
    }
   ],
   "source": [
    "subset_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "time: 11.1 s (started: 2021-04-22 13:12:36 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Build NLP preprocessing pipeline\n",
    "from sparknlp.base import DocumentAssembler\n",
    "document_assembler = DocumentAssembler() \\\n",
    ".setInputCol('text') \\\n",
    ".setOutputCol('document')\n",
    "from sparknlp.annotator import Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    ".setInputCols(['document']) \\\n",
    ".setOutputCol('tokenized') \\\n",
    ".setContextChars(['(', ')']) \\\n",
    ".setSplitChars(['-'])\n",
    "from sparknlp.annotator import Normalizer\n",
    "normalizer = Normalizer() \\\n",
    ".setInputCols(['tokenized']) \\\n",
    ".setOutputCol('normalized') \\\n",
    ".setLowercase(True) \\\n",
    ".setCleanupPatterns(['[^A-Za-z]'])\n",
    "from sparknlp.annotator import LemmatizerModel\n",
    "lemmatizer = LemmatizerModel \\\n",
    ".pretrained() \\\n",
    ".setInputCols(['normalized']) \\\n",
    ".setOutputCol('lemmatized')\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    ".setInputCols(['lemmatized']) \\\n",
    ".setOutputCol('unigrams') \\\n",
    ".setStopWords(nltk_stopwords)\n",
    "from sparknlp.annotator import NGramGenerator\n",
    "ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['unigrams']) \\\n",
    "    .setOutputCol('ngrams') \\\n",
    "    .setN(2) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')\n",
    "from sparknlp.base import Finisher\n",
    "finisher = Finisher() \\\n",
    ".setInputCols(['unigrams', 'ngrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-04-22 13:12:47 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Assemble pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline() \\\n",
    ".setStages([document_assembler,\n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            lemmatizer,\n",
    "            stopwords_cleaner,\n",
    "            ngrammer,\n",
    "            finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 500 ms (started: 2021-04-22 13:12:47 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Fit pipeline\n",
    "processed_reviews = pipeline.fit(subset_df).transform(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text=\"!!!!!\\n\\nWe found ourselves at Foxleys because we were visiting friends in town and they mentioned they had been wanting to try it. I'm so glad they waited!\\n\\nEverything we ordered was *excellent* (though some more than others - get the kale salad and the side ribs for sure), the wine selection was so so - we chose a moderately priced bottle of red which was fine (but apparently forgettable), and the service was friendly, but the waitress/bartender was seemingly (and understandably) overwhelmed.\\n\\nWe are trying to find a way to visit again so we can repeat the night and try the rest of the menu :)\", stars='5.0', categoryIndex=1.0, finished_unigrams=['find', 'foxleys', 'visit', 'friend', 'town', 'mention', 'want', 'try', 'im', 'glad', 'wait', 'everything', 'order', 'excellent', 'though', 'get', 'kale', 'salad', 'side', 'rib', 'sure', 'wine', 'selection', 'choose', 'moderately', 'price', 'bottle', 'red', 'fine', 'apparently', 'forgettable', 'service', 'friendly', 'waitressbartender', 'seemingly', 'understandably', 'overwhelm', 'try', 'find', 'way', 'visit', 'repeat', 'night', 'try', 'rest', 'menu'], finished_ngrams=['find', 'foxleys', 'visit', 'friend', 'town', 'mention', 'want', 'try', 'im', 'glad', 'wait', 'everything', 'order', 'excellent', 'though', 'get', 'kale', 'salad', 'side', 'rib', 'sure', 'wine', 'selection', 'choose', 'moderately', 'price', 'bottle', 'red', 'fine', 'apparently', 'forgettable', 'service', 'friendly', 'waitressbartender', 'seemingly', 'understandably', 'overwhelm', 'try', 'find', 'way', 'visit', 'repeat', 'night', 'try', 'rest', 'menu', 'find_foxleys', 'foxleys_visit', 'visit_friend', 'friend_town', 'town_mention', 'mention_want', 'want_try', 'try_im', 'im_glad', 'glad_wait', 'wait_everything', 'everything_order', 'order_excellent', 'excellent_though', 'though_get', 'get_kale', 'kale_salad', 'salad_side', 'side_rib', 'rib_sure', 'sure_wine', 'wine_selection', 'selection_choose', 'choose_moderately', 'moderately_price', 'price_bottle', 'bottle_red', 'red_fine', 'fine_apparently', 'apparently_forgettable', 'forgettable_service', 'service_friendly', 'friendly_waitressbartender', 'waitressbartender_seemingly', 'seemingly_understandably', 'understandably_overwhelm', 'overwhelm_try', 'try_find', 'find_way', 'way_visit', 'visit_repeat', 'repeat_night', 'night_try', 'try_rest', 'rest_menu'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.8 s (started: 2021-04-22 13:12:48 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Examine one processed review\n",
    "processed_reviews.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2021-04-22 13:13:06 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "(trainingData, testData) = processed_reviews.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251474.40000000002\n",
      "time: 0 ns (started: 2021-04-22 13:13:06 -04:00)\n"
     ]
    }
   ],
   "source": [
    "trainingData_count = subset_count * 0.8\n",
    "print(trainingData_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22min 24s (started: 2021-04-22 13:13:06 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# Count vectorization with minDF and maxDF parameters\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "tfizer = CountVectorizer(inputCol = 'finished_ngrams', outputCol = 'tf_features', \n",
    "                         minDF = 0.01, maxDF = 0.1, vocabSize = int(trainingData_count / 2))\n",
    "\n",
    "tf_model = tfizer.fit(trainingData)\n",
    "tf_result_training = tf_model.transform(trainingData)\n",
    "tf_result_test = tf_model.transform(testData)\n",
    "\n",
    "idfizer = IDF(inputCol = 'tf_features', outputCol = 'tfidf_features')\n",
    "\n",
    "idf_model = idfizer.fit(tf_result_training)\n",
    "tfidf_result_training = idf_model.transform(tf_result_training)\n",
    "tfidf_result_test = idf_model.transform(tf_result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print vocablary length (i.e. # of columns)\n",
    "print(len(tf_model.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "num_topics = 10\n",
    "max_iter = 10\n",
    "lda = LDA(k = num_topics, \n",
    "          maxIter = max_iter, \n",
    "          featuresCol = 'tfidf_features')\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "topics = lda_model \\\n",
    ".describeTopics(num_top_words) \\\n",
    ".withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logistic regression with ridge\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# lr = LogisticRegression(featuresCol = 'tfidf_features', labelCol = 'stars')\n",
    "lr = LogisticRegression(featuresCol = 'tfidf_features', labelCol = 'categoryIndex', \n",
    "                        family = 'binomial', elasticNetParam = 0, regParam = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all parameters\n",
    "{param[0].name: param[1] for param in lr.extractParamMap().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LR model\n",
    "lrModel = lr.fit(tfidf_result_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPredictions_training = lrModel.transform(tfidf_result_training)\n",
    "lrPredictions_test = lrModel.transform(tfidf_result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = 'categoryIndex', predictionCol = 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_training_lr = evaluator.evaluate(lrPredictions_training, {evaluator.metricName: \"accuracy\"})\n",
    "acc_test_lr = evaluator.evaluate(lrPredictions_test, {evaluator.metricName: \"accuracy\"})\n",
    "# f1 = evaluator.evaluate(lrPredictions_training, {evaluator.metricName: \"f1\"})\n",
    "# weightedPrecision = evaluator.evaluate(lrPredictions_training, {evaluator.metricName: \"weightedPrecision\"})\n",
    "# weightedRecall = evaluator.evaluate(lrPredictions_training, {evaluator.metricName: \"weightedRecall\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training accuracy: ' + str(acc_training_lr))\n",
    "print('Test accuracy: ' + str(acc_test_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix = lrModel.coefficientMatrix\n",
    "coef_list = coef_matrix.toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(coef_list).T.sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame(coef_list).T.sort_values(0, ascending = True)\n",
    "for i in range(0, 20):\n",
    "    print(tf_model.vocabulary[coef_df.index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame(coef_list).T.sort_values(0, ascending = False)\n",
    "for i in range(0, 20):\n",
    "    print(tf_model.vocabulary[coef_df.index[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "# rf = RandomForestClassifier(featuresCol = 'tfidf_features', labelCol = 'categoryIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {param[0].name: param[1] for param in rf.extractParamMap().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfModel = rf.fit(tfidf_result_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfPredictions_training = rfModel.transform(tfidf_result_training)\n",
    "# rfPredictions_test = rfModel.transform(tfidf_result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_training_rf = evaluator.evaluate(rfPredictions_training, {evaluator.metricName: \"accuracy\"})\n",
    "# acc_test_rf = evaluator.evaluate(rfPredictions_test, {evaluator.metricName: \"accuracy\"})\n",
    "# # f1 = evaluator.evaluate(rfPredictions_training, {evaluator.metricName: \"f1\"})\n",
    "# # weightedPrecision = evaluator.evaluate(rfPredictions_training, {evaluator.metricName: \"weightedPrecision\"})\n",
    "# # weightedRecall = evaluator.evaluate(rfPredictions_training, {evaluator.metricName: \"weightedRecall\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Training accuracy: ' + str(acc_training_rf))\n",
    "# print('Test accuracy: ' + str(acc_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coef_matrix = rfModel.featureImportances\n",
    "# coef_list = coef_matrix.toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# coef_df = pd.DataFrame(coef_list).sort_values(0, ascending = False)\n",
    "# for i in range(0, 40):\n",
    "#     print(tf_model.vocabulary[coef_df.index[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
